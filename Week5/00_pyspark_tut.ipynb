{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was taken from [here](https://github.com/MoranReznik/PySpark-Reference-Notebook/blob/main/PySpark%20Tutorial.ipynb) and edited. It is the accompanying notebook to Moran Reznik's excellent YouTube video crash course on PySpark [here](https://youtu.be/cZS5xYYIPzk?si=eUfy8cPsKZiJKFN4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is PySpark?\n",
    "PySpark is a Python API for working with Apache Spark. I will first explain what I mean by a \"Python API\" for something and then explain what, specifically, is 'Apache Spark'.\n",
    "\n",
    "What I mean by **'Python API'** is that you can use the syntax and agility of Python to interact with and send commands to a system that is not based, at its core, on Python. \n",
    "\n",
    "With PySpark, you interact with Apache Spark - a system designed for analyzing, modeling and working with immense amounts of data in many computers at the same time. Putting it differently, Apache Spark allows you to run computations in parallel, instead of sequentially. It allows you to divide one incredibly large task into many smaller tasks and run each such task on a different machine. This allows you to accomplish your analysis goals in reasonable time which would not have been possible on a single machine.\n",
    "\n",
    "Usually, we would define the amount of data that suits PySpark as what would not fit in a single machine's permanent storage (let alone RAM).\n",
    "\n",
    "**Important relevant concepts:** \n",
    "1. Distributed computing: When you distribute a task into several smaller tasks that run at the same time. This is what PySpark allows you to do with many machines; but it can also be done on a single machine with several threads, for example.\n",
    "2. Cluster: A network of machines that can take on tasks from a user, interact with one another and return results. These provide the computing resources that PySpark will use to make the computations.\n",
    "3. Resilient Distributed Dataset (RDD): an immutable, distributed collection of data. Unlike DataFrames which we will work with later, it is not tabular and has no data schema. Therefore, for tabular data wrangling, DataFrames allows for more API options and under-the-hood optimizations. Still, you might encounter RDDs as you learn more about Spark, and should be aware of their existence.\n",
    "\n",
    "**Parts of PySpark we will cover:**\n",
    "1. PySpark SQL - contains commands for data processing and manipulation.\n",
    "2. PySpark MLlib - includes a variety of models, model training and related commands.\n",
    "\n",
    "**Spark Architecture:**\n",
    "To send commands and receive results from a cluster, you will need to initiate a Spark 'session'. This object is your gateway for interacting with Spark. Each user of the cluster will have its own Spark session which will allow them to use the cluster in isolation from other users. All sessions communicate with a Spark 'context', which is the master node in the cluster - that is, it assigns each computer in the cluster tasks and coordinates them. Each of the computers in the cluster that perform tasks for a master node is called a 'worker' node. To connect to a worker node, the master node needs to get that node's compute power allocated to it by a cluster 'manager', that is responsible for distributing the cluster's resources. Inside each worker node, there are executor programs that run the tasks - they can run multiple tasks simultaneously, and have their own cache for storing results. So each master node can have multiple worker nodes, which can in turn have multiple tasks running.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a SparkSession object can perform the most common data processing tasks\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('test').getOrCreate() # Will return existing session if one was\n",
    "                                                           # created before and was not closed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.224.184.18:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd180e61bd0>"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dataset:**\n",
    "https://www.kaggle.com/fedesoriano/heart-failure-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv, all columns will be of type string\n",
    "df = spark.read.option('header','true').csv('heart.csv')\n",
    "\n",
    "# Tell pyspark the type of the columns - saves time on large dataset. Note: there are other ways to do this.\n",
    "schema = 'Age INTEGER, Sex STRING, ChestPainType STRING'\n",
    "df = spark.read.csv('heart.csv', schema=schema, header=True)\n",
    "\n",
    "# Let PySpark infer the schema\n",
    "df = spark.read.csv('heart.csv', inferSchema=True, header=True)\n",
    "\n",
    "# Replace nulls with other value at reading time\n",
    "df = spark.read.csv('heart.csv', nullValue='NA')\n",
    "\n",
    "# Save data\n",
    "df.write.format(\"csv\").save(\"heart_save.csv\")\n",
    "\n",
    "# If you want to overwrite the file\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"heart_save.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------+\n",
      "|Age|Sex|ChestPainType|\n",
      "+---+---+-------------+\n",
      "| 40|  M|          ATA|\n",
      "| 49|  F|          NAP|\n",
      "| 37|  M|          ATA|\n",
      "+---+---+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show head of table\n",
    "df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of rows\n",
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|Age|\n",
      "+---+\n",
      "| 40|\n",
      "| 49|\n",
      "| 37|\n",
      "+---+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---+---+\n",
      "|Age|Sex|\n",
      "+---+---+\n",
      "| 40|  M|\n",
      "| 49|  F|\n",
      "| 37|  M|\n",
      "+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show parts of the table\n",
    "df.select('Age').show(3)\n",
    "df.select(['Age','Sex']).show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas DataFrame vs. PySpark DataFrame\n",
    "\n",
    "Both represent a table of data with rows and columns. However, under the hood they are different, as PySpark dataframe needs to support distributed computations. As we move forward, we will see more and more features of it that are not present in Pandas DataFrames. That being said - if you know how to use Pandas, then moving to PySpark will feel like a natural transition.\n",
    "\n",
    "## DAG\n",
    "Directed Acyclic Graph is the way Spark runs computations. When you give it a series of transformations to apply to the dataset, it builds a graph out of those transformations, so it knows what to do. But it does not execute those commands immediately if it does not have to. Rather, it is lazy - it will go through the DAG and apply the transformations only when it must, to provide the required results. This allows better performance, since spark knows what's ahead of a certain computation and gets to optimize the process accordingly.\n",
    "\n",
    "## Transformations vs. Actions\n",
    "In PySpark, there are two types of command: transformations and actions. Transformation commands are added to the DAG, but do not get executed. They transform one DataFrame into another, not changing the input DataFrame. On the other hand, actions make PySpark execute the DAG but do not create a new DataFrame - instead, they output the result of the DAG.\n",
    "\n",
    "## Caching\n",
    "Every time you run a DAG, it will be re-computed from the beginning. That is, the results are not saved in memory. So, if we want to save a result so it won't have to be recomputed, we can use the cache command. Note that this will occupy space in the working node's memory, so be careful with the size of datasets you are caching! By default, the cached DF is stored to RAM, and is unserialized (not converted into a stream of bytes). You can change both of these - store data to hard disk, serialize it, or both!\n",
    "\n",
    "## Collecting\n",
    "Even after caching a DataFrame, it still sits in the worker node's memory. If you want to collect its pieces, assemble them and save them on the master node so you won't have to pull it every time, use the command for collecting. Again, be very careful with this, since the collected file will have to fit in the master node's memory! You will rarely issue this command directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()\n",
    "df.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PySpark DataFrame to Pandas DataFrame\n",
    "pd_df = df.toPandas()\n",
    "\n",
    "# Convert it back\n",
    "spark_df = spark.createDataFrame(pd_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Age=40, Sex='M', ChestPainType='ATA'),\n",
       " Row(Age=49, Sex='F', ChestPainType='NAP'),\n",
       " Row(Age=37, Sex='M', ChestPainType='ATA')]"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first three rows as three row objects, which is how Spark represents single rows from a table.\n",
    "# We will learn more about this in a bit\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- ChestPainType: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the types of columns and whether they are nullable\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Age', 'int'), ('Sex', 'string'), ('ChestPainType', 'string')]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column dtypes as list of tuples\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast a column from one type to another\n",
    "from pyspark.sql.types import FloatType\n",
    "df = df.withColumn(\"Age\",df.Age.cast(FloatType()))\n",
    "df = df.withColumn(\"RestingBP\",df.Age.cast(FloatType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|               Age|         RestingBP|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               918|               918|\n",
      "|   mean|53.510893246187365|53.510893246187365|\n",
      "| stddev|  9.43261650673202|  9.43261650673202|\n",
      "|    min|              28.0|              28.0|\n",
      "|    max|              77.0|              77.0|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute summary statistics\n",
    "df.select(['Age','RestingBP']).describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column or replace an existing one\n",
    "AgeFixed = df['Age'] + 1  # `select` always returns a DataFrame object, but we need a column object\n",
    "df = df.withColumn('AgeFixed', AgeFixed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|          AgeFixed|               Age|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               918|               918|\n",
      "|   mean|54.510893246187365|53.510893246187365|\n",
      "| stddev|  9.43261650673202|  9.43261650673202|\n",
      "|    min|              29.0|              28.0|\n",
      "|    max|              78.0|              77.0|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['AgeFixed','Age']).describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-------------+---------+\n",
      "| Age|Sex|ChestPainType|RestingBP|\n",
      "+----+---+-------------+---------+\n",
      "|40.0|  M|          ATA|     40.0|\n",
      "+----+---+-------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove columns\n",
    "df.drop('AgeFixed').show(1) # Add `df = ` to get the new DataFrame into a variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|40.0|\n",
      "+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename a column\n",
    "df.withColumnRenamed('Age','age').select('age').show(1)\n",
    "\n",
    "# To rename more than a single column, I would suggest a loop\n",
    "name_pairs = [('Age','age'),('Sex','sex')]\n",
    "for old_name, new_name in name_pairs:\n",
    "    df = df.withColumnRenamed(old_name,new_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "| age|sex|\n",
      "+----+---+\n",
      "|40.0|  M|\n",
      "+----+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['age','sex']).show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows that contain any NA\n",
    "df = df.na.drop()\n",
    "df.count()\n",
    "\n",
    "# Drop all rows where all values are NA\n",
    "df = df.na.drop(how='all')\n",
    "\n",
    "# Drop all rows where more than 2 values are NA\n",
    "df = df.na.drop(thresh=2)\n",
    "\n",
    "# Drop all rows where any of the values in specific columns are NAs\n",
    "df = df.na.drop(how='any', subset=['age','sex']) # 'any' is the default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in a specific column with a '?'\n",
    "df = df.na.fill(value='?',subset=['sex'])\n",
    "\n",
    "# Replace NAs with the mean of the column\n",
    "from pyspark.ml.feature import Imputer # In statistics, imputation is the process of\n",
    "                                       # replacing missing data with substituted values\n",
    "imptr = Imputer(inputCols=['age','RestingBP'],\n",
    "                outputCols=['age','RestingBP']).setStrategy('mean') # Can also be 'median', etc.\n",
    "\n",
    "df = imptr.fit(df).transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: float, sex: string, ChestPainType: string, RestingBP: float, AgeFixed: float]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter to adults only and calculate mean\n",
    "df.filter('age > 18')\n",
    "df.where('age > 18')# 'where' is an alias to 'filter'\n",
    "df.where(df['age'] > 18) # third option\n",
    "\n",
    "# Add another condition ('&' means and, '|' means or)\n",
    "df.where((df['age'] > 18) | (df['ChestPainType'] == 'ATA'))\n",
    "\n",
    "# Take every record where the 'ChestPainType' is NOT 'ATA'\n",
    "df.filter(~(df['ChestPainType'] == 'ATA'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "|Age|Sex|ChestPainType|RestingBP|Cholesterol|FastingBS|RestingECG|MaxHR|ExerciseAngina|Oldpeak|ST_Slope|HeartDisease|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "| 40|  M|          ATA|      140|        289|        0|    Normal|  172|             N|    0.0|      Up|           0|\n",
      "| 49|  F|          NAP|      160|        180|        0|    Normal|  156|             N|    1.0|    Flat|           1|\n",
      "| 37|  M|          ATA|      130|        283|        0|        ST|   98|             N|    0.0|      Up|           0|\n",
      "| 48|  F|          ASY|      138|        214|        0|    Normal|  108|             Y|    1.5|    Flat|           1|\n",
      "| 54|  M|          NAP|      150|        195|        0|    Normal|  122|             N|    0.0|      Up|           0|\n",
      "| 39|  M|          NAP|      120|        339|        0|    Normal|  170|             N|    0.0|      Up|           0|\n",
      "| 45|  F|          ATA|      130|        237|        0|    Normal|  170|             N|    0.0|      Up|           0|\n",
      "| 54|  M|          ATA|      110|        208|        0|    Normal|  142|             N|    0.0|      Up|           0|\n",
      "| 37|  M|          ASY|      140|        207|        0|    Normal|  130|             Y|    1.5|    Flat|           1|\n",
      "| 48|  F|          ATA|      120|        284|        0|    Normal|  120|             N|    0.0|      Up|           0|\n",
      "| 37|  F|          NAP|      130|        211|        0|    Normal|  142|             N|    0.0|      Up|           0|\n",
      "| 58|  M|          ATA|      136|        164|        0|        ST|   99|             Y|    2.0|    Flat|           1|\n",
      "| 39|  M|          ATA|      120|        204|        0|    Normal|  145|             N|    0.0|      Up|           0|\n",
      "| 49|  M|          ASY|      140|        234|        0|    Normal|  140|             Y|    1.0|    Flat|           1|\n",
      "| 42|  F|          NAP|      115|        211|        0|        ST|  137|             N|    0.0|      Up|           0|\n",
      "| 54|  F|          ATA|      120|        273|        0|    Normal|  150|             N|    1.5|    Flat|           0|\n",
      "| 38|  M|          ASY|      110|        196|        0|    Normal|  166|             N|    0.0|    Flat|           1|\n",
      "| 43|  F|          ATA|      120|        201|        0|    Normal|  165|             N|    0.0|      Up|           0|\n",
      "| 60|  M|          ASY|      100|        248|        0|    Normal|  125|             N|    1.0|    Flat|           1|\n",
      "| 36|  M|          ATA|      120|        267|        0|    Normal|  160|             N|    3.0|    Flat|           1|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('age > 18').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|new_col|\n",
      "+-------+\n",
      "|   48.2|\n",
      "|   59.0|\n",
      "|   44.6|\n",
      "+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate a string expression into a command\n",
    "from pyspark.sql.functions import expr\n",
    "exp = 'age + 0.2 * AgeFixed'\n",
    "df.withColumn('new_col', expr(exp)).select('new_col').show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|age| avg(HeartDisease)|\n",
      "+---+------------------+\n",
      "| 77|               1.0|\n",
      "| 76|               0.5|\n",
      "| 75|0.6666666666666666|\n",
      "| 74|0.7142857142857143|\n",
      "| 73|               1.0|\n",
      "+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by age\n",
    "disease_by_age = df.groupby('age').mean().select(['age','avg(HeartDisease)'])\n",
    "# Sort values in descending/ascending order\n",
    "from pyspark.sql.functions import desc, asc\n",
    "disease_by_age.orderBy(desc(\"age\")).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|age| avg(HeartDisease)|\n",
      "+---+------------------+\n",
      "| 77|               1.0|\n",
      "| 76|               0.5|\n",
      "| 75|0.6666666666666666|\n",
      "+---+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "disease_by_age = df.groupby('age').mean().select(['age','avg(HeartDisease)'])\n",
    "disease_by_age.orderBy(desc(\"age\")).show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[min(age): int, max(age): int, avg(sex): double]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate to get several statistics for several columns\n",
    "# The available aggregate functions are avg, max, min, sum, count\n",
    "from pyspark.sql import functions as F\n",
    "df.agg(F.min(df['age']),F.max(df['age']),F.avg(df['sex'])).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------+--------+\n",
      "|HeartDisease|min(age)|max(age)|avg(sex)|\n",
      "+------------+--------+--------+--------+\n",
      "|           1|      31|      77|    null|\n",
      "|           0|      28|      76|    null|\n",
      "+------------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('HeartDisease').agg(F.min(df['age']),F.avg(df['sex'])).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|sex|\n",
      "+---+\n",
      "|  M|\n",
      "|  F|\n",
      "+---+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----+----+\n",
      "|older| age|\n",
      "+-----+----+\n",
      "| true|40.0|\n",
      "| true|49.0|\n",
      "+-----+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run an SQL query on the data\n",
    "df.createOrReplaceTempView(\"df\") # tell PySpark how the table will be called in the SQL query\n",
    "spark.sql(\"\"\"SELECT sex from df\"\"\").show(2)\n",
    "\n",
    "# We can also choose columns using SQL syntax with a command that combines '.select()' and '.sql()'\n",
    "df.selectExpr(\"age >= 40 as older\", \"age\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|age|  M|  F|\n",
      "+---+---+---+\n",
      "| 31|  1|  1|\n",
      "| 65| 17|  4|\n",
      "| 53| 27|  6|\n",
      "+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('age').pivot('sex', (\"M\", \"F\")).count().show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "|sex|true|false|\n",
      "+---+----+-----+\n",
      "|  F| 174|   19|\n",
      "|  M| 664|   61|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pivot - Note: this is a very expensive operation. You can mitigate this by\n",
    "# specifying the values to piivot on, as we've done below.\n",
    "df.selectExpr(\"age >= 40 as older\", \"age\",'sex').groupBy(\"sex\")\\\n",
    "                    .pivot(\"older\", (\"true\", \"false\")).count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+\n",
      "|age|MaxHR|Cholesterol|\n",
      "+---+-----+-----------+\n",
      "| 40|  172|        289|\n",
      "| 49|  156|        180|\n",
      "| 37|   98|        283|\n",
      "| 48|  108|        214|\n",
      "+---+-----+-----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['age','MaxHR','Cholesterol']).show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------------+-----+\n",
      "|Age|Cholesterol|        Fvec|MaxHR|\n",
      "+---+-----------+------------+-----+\n",
      "| 40|        289|[40.0,289.0]|  172|\n",
      "| 49|        180|[49.0,180.0]|  156|\n",
      "| 37|        283|[37.0,283.0]|   98|\n",
      "+---+-----------+------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# divide dataset to training features and target\n",
    "X_column_names = ['Age','Cholesterol']\n",
    "target_colum_name = ['MaxHR']\n",
    "\n",
    "# Convert feature columns into a columns where the values are feature vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "v_asmblr = VectorAssembler(inputCols=X_column_names, outputCol='Fvec')\n",
    "df = v_asmblr.transform(df)\n",
    "X = df.select(['Age','Cholesterol','Fvec','MaxHR'])\n",
    "X.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide dataset into training and testing sets\n",
    "trainset, testset = X.randomSplit([0.8,0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9981223334822935,0.04620857054247365]\n",
      "181.31618579521276\n"
     ]
    }
   ],
   "source": [
    "# Predict using linear regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "model = LinearRegression(featuresCol='Fvec', labelCol='MaxHR')\n",
    "model = model.fit(trainset)\n",
    "print(model.coefficients)\n",
    "print(model.intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------------+-----+------------------+\n",
      "|Age|Cholesterol|        Fvec|MaxHR|        prediction|\n",
      "+---+-----------+------------+-----+------------------+\n",
      "| 28|        132|[28.0,132.0]|  185|159.46829176931507|\n",
      "| 30|        237|[30.0,237.0]|  170|162.32394700931022|\n",
      "| 34|        210|[34.0,210.0]|  192|157.08382627073425|\n",
      "+---+-----------+------------+-----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "model.evaluate(testset).predictions.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle categorical features with ordinal indexing\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indxr = StringIndexer(inputCol='ChestPainType', outputCol='ChestPainTypeInxed')\n",
    "indxr.fit(df).transform(df).select('ChestPainTypeInxed').show(3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

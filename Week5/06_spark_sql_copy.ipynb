{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b3e0f2d-510d-4f98-81e0-38bc14b2982f",
   "metadata": {},
   "source": [
    "# Creating A Local Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8dd2a-2290-479f-bdae-d499624451d3",
   "metadata": {},
   "source": [
    "Until now, we've been creating our spark sessions using:\n",
    "``` python\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"NYTaxi\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "Now, we will be starting a cluster locally using Spark's \"Standalone Mode\" and connecting to it. We are loosely following the instructions [here](https://spark.apache.org/docs/latest/spark-standalone.html). The referenced scripts are all in `$SPARK_HOME/sbin`. We started the master with `start-master.sh`.  \n",
    "This time, we specify the master URL which we can either get from the `.out` log file or from the MasterWebUI which is now available on port `8080` (as opposed to `4040` using the 'local' method above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e6e92c-1847-4517-8453-301309c37a01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "286116cf-d5e9-4dce-96df-343e20e05cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/31 21:07:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://dezc-vm2.us-west1-a.c.brilliant-vent-400717.internal:7077\") \\\n",
    "    .appName(\"NYTaxi\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "761da168-b439-47d5-b017-e7a52d6d350c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://dezc-vm2.us-west1-a.c.brilliant-vent-400717.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://dezc-vm2.us-west1-a.c.brilliant-vent-400717.internal:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NYTaxi</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8e7c75d3d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb7994-9495-4beb-af6a-eb67d669fd29",
   "metadata": {},
   "source": [
    "Let's run something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c349f85-2989-454d-98dd-179bbc73cf46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/31 21:07:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/10/31 21:07:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: integer (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet(\"data/pq/green/*/*\")\n",
    "df_green.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8181b944-3941-4c2f-948c-2418a39ef7a3",
   "metadata": {},
   "source": [
    "We started a job without starting any workers first, hence the warning messages. We needed to start a cluster manually using `start-worker.sh <master-spark-URL>` first. Then the task ran and we got output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a15a91e-50a0-426a-88fe-76c37b9b535f",
   "metadata": {},
   "source": [
    "Next, we will convert this notebook to a script and modify it to accept parameters. I'm actually going to remove everything below this cell now, but you can see the resulting Python script, [06_spark_sql_script.py](06_spark_sql_script.py), under the same folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cd682-2b83-42e9-8e51-fc7ef8e4bfb4",
   "metadata": {},
   "source": [
    "> *Note:* Before we attempt to run the script, we have to free up the resources this notebook is taking in Spark. Until we do, we will see the same warnings we saw in the code cell above, because the first application (this notebook) will take up all the resources because we never specified how many executors it needed so it took everything that was available. The first application can be `kill`ed from the MasterWebUI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

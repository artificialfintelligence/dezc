{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fde351c-3c32-43df-ad82-09dd8c7e3efa",
   "metadata": {},
   "source": [
    "# Working with RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bfac4af3-c242-4978-a543-83f97247ca99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2302514-0717-42d6-aa7c-dc61c156a35b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/28 22:28:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"NYTaxi\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b86ba71-ea15-4c51-8fa2-ba637b58d937",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet('data/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aa7dfc-4345-4b28-879f-399d6f86d2a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "We want to implement the following, but this time with RDDs instead of the convenient API that DataFrames provide.\n",
    "\n",
    "``` sql\n",
    "SELECT \n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dba543e2-f041-4db9-986c-0960b311e933",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[7] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f355d78-7048-4b49-93d2-9179fddf5405",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 13, 10, 15), lpep_dropoff_datetime=datetime.datetime(2020, 1, 23, 13, 38, 16), store_and_fwd_flag='N', RatecodeID=1, PULocationID=74, DOLocationID=130, passenger_count=1, trip_distance=12.77, fare_amount=36.0, extra=0.0, mta_tax=0.5, tip_amount=2.05, tolls_amount=6.12, ehail_fee=None, improvement_surcharge=0.3, total_amount=44.97, payment_type=1, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=None, lpep_pickup_datetime=datetime.datetime(2020, 1, 20, 15, 9), lpep_dropoff_datetime=datetime.datetime(2020, 1, 20, 15, 46), store_and_fwd_flag=None, RatecodeID=None, PULocationID=67, DOLocationID=39, passenger_count=None, trip_distance=8.0, fare_amount=29.9, extra=2.75, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=33.45, payment_type=None, trip_type=None, congestion_surcharge=None),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 15, 20, 23, 41), lpep_dropoff_datetime=datetime.datetime(2020, 1, 15, 20, 31, 18), store_and_fwd_flag='N', RatecodeID=1, PULocationID=260, DOLocationID=157, passenger_count=1, trip_distance=1.27, fare_amount=7.0, extra=0.5, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=8.3, payment_type=2, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 5, 16, 32, 26), lpep_dropoff_datetime=datetime.datetime(2020, 1, 5, 16, 40, 51), store_and_fwd_flag='N', RatecodeID=1, PULocationID=82, DOLocationID=83, passenger_count=1, trip_distance=1.25, fare_amount=7.5, extra=0.0, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=8.3, payment_type=2, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 29, 19, 22, 42), lpep_dropoff_datetime=datetime.datetime(2020, 1, 29, 19, 31, 2), store_and_fwd_flag='N', RatecodeID=1, PULocationID=166, DOLocationID=42, passenger_count=1, trip_distance=1.84, fare_amount=8.0, extra=1.0, mta_tax=0.5, tip_amount=2.94, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=12.74, payment_type=1, trip_type=1, congestion_surcharge=0.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd512192-3524-4edb-84ff-e9dedecc183c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select only the columns we need\n",
    "rdd = df_green \\\n",
    "    .select(\"lpep_pickup_datetime\", \"PULocationID\", \"total_amount\") \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5ba18a7-5b84-4298-b46e-5466aa08086b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 13, 10, 15), PULocationID=74, total_amount=44.97),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 20, 15, 9), PULocationID=67, total_amount=33.45),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 15, 20, 23, 41), PULocationID=260, total_amount=8.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 5, 16, 32, 26), PULocationID=82, total_amount=8.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 29, 19, 22, 42), PULocationID=166, total_amount=12.74)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876895bb-e442-4397-aa97-82fa38e6fae2",
   "metadata": {},
   "source": [
    "## Implementing the `WHERE` Clause with `filter()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81f37fd6-737f-47cc-ab96-f4c0f68c37a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[16] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = datetime(year=2020, month=1, day=1)\n",
    "\n",
    "def filter_date_outliers(row):\n",
    "    return row.lpep_pickup_datetime >= start_date\n",
    "\n",
    "rdd \\\n",
    "    .filter(filter_date_outliers)\n",
    "    # Or just\n",
    "    # .filter(lambda row: row.lpep_pickup_datetime >= start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76103bf-bc20-4698-b443-063f83681a82",
   "metadata": {},
   "source": [
    "## Preparing for Grouping with `map()` and Implementing the `GROUP BY` Clause with `reduce()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeedd0fc-b5e1-44f7-a7c3-ce9535b9470b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 13, 10, 15), PULocationID=74, total_amount=44.97)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_row = rdd.take(1)[0]\n",
    "sample_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59434d9a-834e-4065-9e89-5382177a5130",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 1, 23, 13, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We only want the hour (see `date_trunc` in the SQL query)\n",
    "sample_row.lpep_pickup_datetime.replace(minute=0, second=0, microsecond=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39461a10-23bd-46f5-ba43-b350da254060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_for_grouping(row):\n",
    "    # (hour, zone) comprises our composite key \n",
    "    hour = row.lpep_pickup_datetime.replace(minute=0, second=0, microsecond=0)\n",
    "    zone = row.PULocationID\n",
    "    \n",
    "    # (amount, count) makes up our value\n",
    "    amount = row.total_amount\n",
    "    count = 1\n",
    "    \n",
    "    key = (hour, zone)\n",
    "    value = (amount, count)\n",
    "    \n",
    "    return (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a737ae7e-a71d-4ef9-a7f8-b0b712e50655",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((datetime.datetime(2020, 1, 23, 13, 0), 74), (44.97, 1)),\n",
       " ((datetime.datetime(2020, 1, 20, 15, 0), 67), (33.45, 1)),\n",
       " ((datetime.datetime(2020, 1, 15, 20, 0), 260), (8.3, 1)),\n",
       " ((datetime.datetime(2020, 1, 5, 16, 0), 82), (8.3, 1)),\n",
       " ((datetime.datetime(2020, 1, 29, 19, 0), 166), (12.74, 1))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd \\\n",
    "    .filter(filter_date_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d928b857-109e-432f-9538-f7840bb1055d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_calc(left_value, right_value):\n",
    "    left_amount, left_count = left_value\n",
    "    right_amount, right_count = right_value\n",
    "    \n",
    "    output_amount = left_amount + right_amount\n",
    "    output_count = left_count + right_count\n",
    "    \n",
    "    return (output_amount, output_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f63cc00a-6d7b-43fe-af2a-71f669d10425",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((datetime.datetime(2020, 1, 15, 20, 0), 260), (163.90000000000003, 14)),\n",
       " ((datetime.datetime(2020, 1, 29, 19, 0), 166), (695.0099999999999, 45)),\n",
       " ((datetime.datetime(2020, 1, 16, 8, 0), 41), (736.1399999999996, 54)),\n",
       " ((datetime.datetime(2020, 1, 4, 20, 0), 129), (583.27, 38)),\n",
       " ((datetime.datetime(2020, 1, 2, 8, 0), 66), (197.69, 10))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd \\\n",
    "    .filter(filter_date_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(custom_calc) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac870b3-bbad-40ba-a507-a926403e8ed1",
   "metadata": {},
   "source": [
    "That took considerably longer, because it had to go through and aggregate the entire dataset! Now let's just format the output (the nested tuples aren't very nice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47d8d3a5-3c67-437a-8c08-d5d6a9c391e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unwrap(row):\n",
    "    return (row[0][0], row[0][1], row[1][0], row[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9163e51a-d1bf-4e85-98d8-2f8e8df96e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(datetime.datetime(2020, 1, 15, 20, 0), 260, 163.90000000000003, 14),\n",
       " (datetime.datetime(2020, 1, 29, 19, 0), 166, 695.0099999999999, 45),\n",
       " (datetime.datetime(2020, 1, 16, 8, 0), 41, 736.1399999999996, 54),\n",
       " (datetime.datetime(2020, 1, 4, 20, 0), 129, 583.27, 38),\n",
       " (datetime.datetime(2020, 1, 2, 8, 0), 66, 197.69, 10)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd \\\n",
    "    .filter(filter_date_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(custom_calc) \\\n",
    "    .map(unwrap) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b25c3d-3f8f-4be6-b31c-0cca3851733a",
   "metadata": {},
   "source": [
    "And finally, to turn it into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f1cf4b8-5384-4d48-8b48-9a1449afde81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+------------------+---+\n",
      "|                 _1| _2|                _3| _4|\n",
      "+-------------------+---+------------------+---+\n",
      "|2020-01-15 20:00:00|260|163.90000000000003| 14|\n",
      "|2020-01-29 19:00:00|166| 695.0099999999999| 45|\n",
      "|2020-01-16 08:00:00| 41| 736.1399999999996| 54|\n",
      "|2020-01-04 20:00:00|129|            583.27| 38|\n",
      "|2020-01-02 08:00:00| 66|            197.69| 10|\n",
      "|2020-01-03 09:00:00| 61|            142.21|  9|\n",
      "|2020-01-17 21:00:00|236|              33.6|  4|\n",
      "|2020-01-12 12:00:00| 82|            290.41| 14|\n",
      "|2020-01-28 16:00:00|197| 831.4399999999998| 18|\n",
      "|2020-01-10 22:00:00| 95| 407.7100000000002| 37|\n",
      "|2020-01-10 01:00:00|215|            109.69|  2|\n",
      "|2020-01-07 18:00:00| 25| 554.2900000000001| 37|\n",
      "|2020-01-18 07:00:00| 55|              48.3|  1|\n",
      "|2020-01-28 09:00:00|166| 473.0200000000002| 36|\n",
      "|2020-01-12 15:00:00| 82| 265.7900000000001| 29|\n",
      "|2020-01-10 20:00:00| 66|            405.88| 21|\n",
      "|2020-01-31 15:00:00| 43|345.58000000000004| 19|\n",
      "|2020-01-31 21:00:00| 41| 588.1600000000001| 40|\n",
      "|2020-01-25 18:00:00| 65| 457.0600000000001| 28|\n",
      "|2020-01-26 14:00:00|166| 301.7900000000001| 26|\n",
      "+-------------------+---+------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd \\\n",
    "    .filter(filter_date_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(custom_calc) \\\n",
    "    .map(unwrap) \\\n",
    "    .toDF() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f60d420-96fb-4f24-a0ac-de59050e9471",
   "metadata": {},
   "source": [
    "Oh no! Not surprisingly, we have lost our column names and schema! Fret not, as we will restore them with named tuples 😎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c9be64e-2219-4d0f-bec2-2534a5419f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcessedRow = namedtuple(\"ProcessedRow\", [\"hour\", \"zone\", \"revenue\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "625b746c-6588-4ff3-9d38-a14237ccc3be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unwrap_and_add_col_names(row):\n",
    "    return ProcessedRow(\n",
    "        hour=row[0][0],\n",
    "        zone=row[0][1],\n",
    "        revenue=row[1][0],\n",
    "        count=row[1][1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c4c6f88-3925-4741-818f-88e76413a85d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result = rdd \\\n",
    "    .filter(filter_date_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(custom_calc) \\\n",
    "    .map(unwrap_and_add_col_names) \\\n",
    "    .toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2332c13-6956-465b-8aaf-890e296601c4",
   "metadata": {},
   "source": [
    "We didn't do `.show()` yet Spark seems to be processing the entire dataset. Why? This time `.toDF()` is the culprit. Since we have not specified a schema, Spark has to go through every single row to infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe673601-0105-440b-94c5-98b90950120e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hour', TimestampType(), True), StructField('zone', LongType(), True), StructField('revenue', DoubleType(), True), StructField('count', LongType(), True)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8233eb0f-ebad-4968-be27-9871839f1a7f",
   "metadata": {},
   "source": [
    "Let's provide the schema..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e66bb9d0-5499-439d-b85b-7962fab3bef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_schema = types.StructType([\n",
    "    types.StructField('hour', types.TimestampType(), True), \n",
    "    types.StructField('zone', types.IntegerType(), True), \n",
    "    types.StructField('revenue', types.DoubleType(), True), \n",
    "    types.StructField('count', types.IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a621020e-c755-4c9f-8f9a-5bb1b4745222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_result = rdd \\\n",
    "    .filter(filter_date_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(custom_calc) \\\n",
    "    .map(unwrap_and_add_col_names) \\\n",
    "    .toDF(schema=result_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8848ee85-d0ad-4fac-9a7e-cfb9814f671b",
   "metadata": {},
   "source": [
    "Now unless we do `.show()` or another \"eager\" action, it will be a \"lazy\" transformation. Speaking of eager actions, let's save the resulting dataset to a Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38e2eece-abdf-4d35-9d8c-5bf9f8b65920",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.write.parquet(\"data/report/green_agg_hourly_by_zone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2b284-a634-46fc-a360-0eda5fad64af",
   "metadata": {},
   "source": [
    "If we look at the execution graph, we'll see that it was a two-stage one, as before. The \"prep\" step was stage 1 and then `.reduceByKey()` does reshuffling (the second stage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9df95cb2-a77d-4499-b345-86911bfe9132",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\n",
      "part-00000-919726c8-2ae2-44d9-ae94-3447dcccc59e-c000.snappy.parquet\n",
      "part-00001-919726c8-2ae2-44d9-ae94-3447dcccc59e-c000.snappy.parquet\n",
      "part-00002-919726c8-2ae2-44d9-ae94-3447dcccc59e-c000.snappy.parquet\n",
      "part-00003-919726c8-2ae2-44d9-ae94-3447dcccc59e-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls data/report/green_agg_hourly_by_zone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd5dfa1-bfc5-4048-952b-6112f4f6d77b",
   "metadata": {},
   "source": [
    "## The `mapPartitions()` Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b66c71-f697-40c6-aebd-20413d97341c",
   "metadata": {},
   "source": [
    "It is similar to `map()` but applies a function to a whole partition of data. Very convenient when we have a huge amount of data split into smaller, more manageable partitions. Effectively \"chunking\" the data. Most notably applicable in machine learning (e.g. inference from a trained model). Let's see it in action. Imagine we have an ML model that predicts the duration of a trip given a handful of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79e864cc-87e6-49fe-bbcb-c792a90b3405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [\"VendorID\", \"lpep_pickup_datetime\", \"PULocationID\", \"DOLocationID\", \"trip_distance\"]\n",
    "duration_rdd = df_green \\\n",
    "    .select(features) \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9488c37f-a589-4de0-ad27-cd0044b7f66a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_model_bactched(partition):\n",
    "    return [1] # Just for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbdfd8d-0f12-46b4-ba43-41dfcdd0b46e",
   "metadata": {},
   "source": [
    "Note that the function above _must_ return an iterable object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d4eb9b5-b491-4a81-b8ef-51c823e9727d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_rdd.mapPartitions(apply_model_bactched).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef45df64-0972-479f-9699-2bdd3305caff",
   "metadata": {},
   "source": [
    "Why did we get this output? Because there were four partitions. The `collect()` function \"flattens\" the output, i.e. the original output would have been `[[1], [1], [1], [1]]`.  \n",
    "Now let's see if we can take a peek inside the partitions with a sneaky \"model\" implementation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3b304a8-8ddf-427a-aea5-45e69d33dfc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_model_bactched(partition):\n",
    "    # We cannot do `cnt = len(partition)` because partitions are iterator obejects. Instead:\n",
    "    \n",
    "    cnt = 0\n",
    "    for row in partition:\n",
    "        cnt += 1\n",
    "    \n",
    "    return [cnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab78eb99-7da1-4878-921e-94f7811be569",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1141148, 438057, 432402, 292910]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_rdd.mapPartitions(apply_model_bactched).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a03550-18a4-4142-9a33-d65214bcc55f",
   "metadata": {},
   "source": [
    "We can see that our partitions are not very balanced at all. Ideally, we would want them to be balanced. To mitigate this, we could perform \"repartitioning\". But repartitioning is an expensive operation so we'd have to be smart about when and how we do it. While an important concept, repartitioning is outside the scope of this lesson so we won't discuss it any further right now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd714b6-8d46-4f50-8e27-5f29514bf2a6",
   "metadata": {},
   "source": [
    "As data scientists, we like 🐼s. So let's convert our Spark DataFrame to a Pandas one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "644df882-942b-44a6-a0b0-de2e6a225b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_model_bactched(rows):\n",
    "    df = pd.DataFrame(rows, columns=features)\n",
    "    cnt = len(df)\n",
    "    return [cnt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f8da4-35b2-4180-9186-ab6ebdaf6522",
   "metadata": {},
   "source": [
    "**Note:** An important caveat to the above implementation is that the casting to `pd.DataFrame` will materialize the entire partition in memory. So your exectors need to have enough memory. If they don't, you'll have to, you guessed it, repartition (or otherwise manually \"chunk\" your data somehow, say, using [`islice()` from `itertools`](https://docs.python.org/3/library/itertools.html#itertools.islice))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a06d7d9-548d-48fe-99ab-6e7d4a221025",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1141148, 438057, 432402, 292910]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_rdd.mapPartitions(apply_model_bactched).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "111af763-a3ab-41b4-ab6d-9a74ac7aee3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load our fancy machine learning model\n",
    "# model = ...\n",
    "\n",
    "def model_predict(df):\n",
    "    # y_pred = model.predict(df)\n",
    "    y_pred = df.trip_distance * 5 # Truly state of the art\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9698179-bc8c-439b-8c7a-7ad385ac068e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_model_bactched(rows):\n",
    "    df = pd.DataFrame(rows, columns=features)\n",
    "    predictions = model_predict(df)\n",
    "    df[\"predicted_duration\"] = predictions\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7878df6e-fde5-4b42-9406-3244bac5ca0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Pandas(Index=0, VendorID=2.0, lpep_pickup_datetime=Timestamp('2020-01-23 13:10:15'), PULocationID=74, DOLocationID=130, trip_distance=12.77, predicted_duration=63.849999999999994),\n",
       " Pandas(Index=1, VendorID=nan, lpep_pickup_datetime=Timestamp('2020-01-20 15:09:00'), PULocationID=67, DOLocationID=39, trip_distance=8.0, predicted_duration=40.0),\n",
       " Pandas(Index=2, VendorID=2.0, lpep_pickup_datetime=Timestamp('2020-01-15 20:23:41'), PULocationID=260, DOLocationID=157, trip_distance=1.27, predicted_duration=6.35),\n",
       " Pandas(Index=3, VendorID=2.0, lpep_pickup_datetime=Timestamp('2020-01-05 16:32:26'), PULocationID=82, DOLocationID=83, trip_distance=1.25, predicted_duration=6.25),\n",
       " Pandas(Index=4, VendorID=2.0, lpep_pickup_datetime=Timestamp('2020-01-29 19:22:42'), PULocationID=166, DOLocationID=42, trip_distance=1.84, predicted_duration=9.200000000000001),\n",
       " Pandas(Index=5, VendorID=2.0, lpep_pickup_datetime=Timestamp('2020-01-15 11:07:42'), PULocationID=179, DOLocationID=223, trip_distance=0.76, predicted_duration=3.8),\n",
       " Pandas(Index=6, VendorID=2.0, lpep_pickup_datetime=Timestamp('2020-01-16 08:22:29'), PULocationID=41, DOLocationID=237, trip_distance=3.32, predicted_duration=16.599999999999998),\n",
       " Pandas(Index=7, VendorID=2.0, lpep_pickup_datetime=Timestamp('2020-01-28 17:05:28'), PULocationID=75, DOLocationID=161, trip_distance=2.21, predicted_duration=11.05),\n",
       " Pandas(Index=8, VendorID=1.0, lpep_pickup_datetime=Timestamp('2020-01-22 14:51:37'), PULocationID=152, DOLocationID=166, trip_distance=0.9, predicted_duration=4.5),\n",
       " Pandas(Index=9, VendorID=2.0, lpep_pickup_datetime=Timestamp('2020-01-31 10:25:04'), PULocationID=75, DOLocationID=234, trip_distance=6.1, predicted_duration=30.5)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_rdd.mapPartitions(apply_model_bactched).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72d67171-c7ab-493f-859b-3f1ceffab536",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_predictions = duration_rdd \\\n",
    "    .mapPartitions(apply_model_bactched) \\\n",
    "    .toDF() \\\n",
    "    .drop(\"Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "980199c9-6a7f-465a-8849-108e6ee7a049",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+------------+-------------+------------------+\n",
      "|VendorID|lpep_pickup_datetime|PULocationID|DOLocationID|trip_distance|predicted_duration|\n",
      "+--------+--------------------+------------+------------+-------------+------------------+\n",
      "|     2.0|                  {}|          74|         130|        12.77|63.849999999999994|\n",
      "|     NaN|                  {}|          67|          39|          8.0|              40.0|\n",
      "|     2.0|                  {}|         260|         157|         1.27|              6.35|\n",
      "|     2.0|                  {}|          82|          83|         1.25|              6.25|\n",
      "|     2.0|                  {}|         166|          42|         1.84| 9.200000000000001|\n",
      "|     2.0|                  {}|         179|         223|         0.76|               3.8|\n",
      "|     2.0|                  {}|          41|         237|         3.32|16.599999999999998|\n",
      "|     2.0|                  {}|          75|         161|         2.21|             11.05|\n",
      "|     1.0|                  {}|         152|         166|          0.9|               4.5|\n",
      "|     2.0|                  {}|          75|         234|          6.1|              30.5|\n",
      "|     2.0|                  {}|          75|          41|         1.74|               8.7|\n",
      "|     2.0|                  {}|         260|         226|         1.18|5.8999999999999995|\n",
      "|     1.0|                  {}|         129|         129|          2.2|              11.0|\n",
      "|     2.0|                  {}|          74|         126|         3.04|              15.2|\n",
      "|     2.0|                  {}|          61|          61|         0.85|              4.25|\n",
      "|     2.0|                  {}|          66|         164|         5.06|25.299999999999997|\n",
      "|     2.0|                  {}|           7|         179|         1.57|7.8500000000000005|\n",
      "|     2.0|                  {}|          74|         243|          6.8|              34.0|\n",
      "|     2.0|                  {}|          66|          97|         1.06| 5.300000000000001|\n",
      "|     2.0|                  {}|          61|         225|         1.23|              6.15|\n",
      "+--------+--------------------+------------+------------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3227efe8-7974-44f7-8b9f-5f8bded4e9c8",
   "metadata": {},
   "source": [
    "Ideally we should have specified the schema so Spark doesn't infer it. But we've done that a number of times now so you've got the idea. (Also I'm not sure what's going on with `lpep_pickup_datetime` above...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40f0488e-6672-4cd8-a915-a85fe9ddf5d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|predicted_duration|\n",
      "+------------------+\n",
      "|63.849999999999994|\n",
      "|              40.0|\n",
      "|              6.35|\n",
      "|              6.25|\n",
      "| 9.200000000000001|\n",
      "|               3.8|\n",
      "|16.599999999999998|\n",
      "|             11.05|\n",
      "|               4.5|\n",
      "|              30.5|\n",
      "|               8.7|\n",
      "|5.8999999999999995|\n",
      "|              11.0|\n",
      "|              15.2|\n",
      "|              4.25|\n",
      "|25.299999999999997|\n",
      "|7.8500000000000005|\n",
      "|              34.0|\n",
      "| 5.300000000000001|\n",
      "|              6.15|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_predictions.select(\"predicted_duration\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

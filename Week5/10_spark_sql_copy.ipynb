{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b3e0f2d-510d-4f98-81e0-38bc14b2982f",
   "metadata": {},
   "source": [
    "# Creating A Local Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8dd2a-2290-479f-bdae-d499624451d3",
   "metadata": {},
   "source": [
    "Until now, we've been creating our spark sessions using:\n",
    "``` python\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"NYTaxi\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "Now, we will be starting a cluster locally using Spark's \"Standalone Mode\" and connecting to it. We are loosely following the instructions [here](https://spark.apache.org/docs/latest/spark-standalone.html). The referenced scripts are all in `$SPARK_HOME/sbin`. We started the master with `start-master.sh`.  \n",
    "This time, we specify the master URL which we can either get from the `.out` log file or from the MasterWebUI which is now available on port `8080` (as opposed to `4040` using the 'local' method above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e6e92c-1847-4517-8453-301309c37a01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "286116cf-d5e9-4dce-96df-343e20e05cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/31 21:07:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://dezc-vm2.us-west1-a.c.brilliant-vent-400717.internal:7077\") \\\n",
    "    .appName(\"NYTaxi\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "761da168-b439-47d5-b017-e7a52d6d350c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://dezc-vm2.us-west1-a.c.brilliant-vent-400717.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://dezc-vm2.us-west1-a.c.brilliant-vent-400717.internal:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NYTaxi</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8e7c75d3d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb7994-9495-4beb-af6a-eb67d669fd29",
   "metadata": {},
   "source": [
    "Let's run something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c349f85-2989-454d-98dd-179bbc73cf46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/31 21:07:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/10/31 21:07:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: integer (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet(\"data/pq/green/*/*\")\n",
    "df_green.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8181b944-3941-4c2f-948c-2418a39ef7a3",
   "metadata": {},
   "source": [
    "We started a job without starting any workers first, hence the warning messages. We needed to start a cluster manually using `start-worker.sh <master-spark-URL>` first. Then the task ran and we got output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a15a91e-50a0-426a-88fe-76c37b9b535f",
   "metadata": {},
   "source": [
    "Next, we will convert this notebook to a script and modify it to accept parameters. I'm actually going to remove everything below this cell now, but you can see the resulting Python script, [10_spark_sql_script.py](10_spark_sql_script.py), under the same folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cd682-2b83-42e9-8e51-fc7ef8e4bfb4",
   "metadata": {},
   "source": [
    "> **Note 1:** Before we attempt to run the script, we have to free up the resources this notebook is taking in Spark. Until we do, we will see the same warnings we saw in the code cell above, because the first application (this notebook) will take up all the resources because we never specified how many executors it needed so it took everything that was available. The first application can be `kill`ed from the MasterWebUI. (Although it may have been killed once we shut down the Jupyter kernel)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af21c0d2-d06a-451b-aa9e-f8b59205ed9a",
   "metadata": {},
   "source": [
    "## The Evolution of the Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377fdbc2-bc21-4ba4-83c8-04f93b622368",
   "metadata": {},
   "source": [
    "The first itearion of the script essentially had the exact same logic, only with added arument-parsing for flexibility. We ran it with:\n",
    "``` bash\n",
    "python 10_spark_sql_script.py \\\n",
    "    --input_green=data/pq/green/2020/*/ \\\n",
    "    --input_yellow=data/pq/yellow/2020/*/ \\\n",
    "    --output=data/report-2020\n",
    "```\n",
    "which resulted in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d268e27-47d5-42f9-9d88-058cd53e6b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mdata/report-2020/\u001b[00m\n",
      "├── _SUCCESS\n",
      "└── part-00000-c82ba606-5ad2-4ce7-86a1-f72094cf6aad-c000.snappy.parquet\n",
      "\n",
      "0 directories, 2 files\n"
     ]
    }
   ],
   "source": [
    "!tree data/report-2020/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7278a0fc-e591-4981-b64a-05368b47f17b",
   "metadata": {},
   "source": [
    "But hard-coding the master like this\n",
    "```python\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://dezc-vm2.us-west1-a.c.brilliant-vent-400717.internal:7077\") \\\n",
    "    .appName(\"NYTaxi\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "is not ideal in practice. What if we wanted to run this using Airflow? What if we wanted more flexibility, specifying the number of executors we wanted and how much RAM we wanted them to have? That is why we have removed the line that specifies and hardcodes the Spark master above (`.master(...)`) in the final script. We then use the `spark-submit` script to submit our job to the cluster. See the full documentation [here](https://spark.apache.org/docs/latest/submitting-applications.html).\n",
    "``` bash\n",
    "spark-submit \\\n",
    "    --master=\"spark://dezc-vm2.us-west1-a.c.brilliant-vent-400717.internal:7077\" \\\n",
    "    10_spark_sql_script.py \\\n",
    "        --input_green=data/pq/green/2021/*/ \\\n",
    "        --input_yellow=data/pq/yellow/2021/*/ \\\n",
    "        --output=data/report-2021\n",
    "```\n",
    "\n",
    "We see a lot more output to the console using `spark-submit`, which is good because we didn't add much logging to our script!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "854d72cd-5978-4d5b-98e2-11e7bbe294b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mdata/report-2021/\u001b[00m\n",
      "├── _SUCCESS\n",
      "└── part-00000-8b6a8b04-408c-4c4e-bb23-d8a49767d31e-c000.snappy.parquet\n",
      "\n",
      "0 directories, 2 files\n"
     ]
    }
   ],
   "source": [
    "!tree data/report-2021/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b18adb-4b62-41ac-b5ea-641be6a6c025",
   "metadata": {},
   "source": [
    "Don't forget to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3f95f10-e192-4428-a27c-e388a0d8b2a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping org.apache.spark.deploy.master.Master\n"
     ]
    }
   ],
   "source": [
    "!$SPARK_HOME/sbin/stop-master.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521fc9e3-9508-4482-85ef-adfa3550bb02",
   "metadata": {},
   "source": [
    "and:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5726c9d9-876b-4f42-ab24-e00d6373c281",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping org.apache.spark.deploy.worker.Worker\n"
     ]
    }
   ],
   "source": [
    "!$SPARK_HOME/sbin/stop-worker.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045feda-45c2-4dab-9d15-8ca6458dfc13",
   "metadata": {},
   "source": [
    "In the next part, we're going to create a Spark cluster on GCP and use it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
